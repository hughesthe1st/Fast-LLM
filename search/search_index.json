{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fast-LLM","text":"<p>Welcome to Fast-LLM, an innovative library designed for training large language models with an emphasis on speed, flexibility, and convenience. Developed by ServiceNow Research's Foundation Models Lab, Fast-LLM is tailored to meet the rigorous demands of enterprise AI solutions, providing a foundation for our bespoke generative AI applications.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Speed: Fast-LLM delivers unparalleled training throughput, achieving speeds up to 4,000 tokens/s/GPU for Mixtral-8x7B and nearly 9,000 tokens/s/GPU for Mistral-7B, facilitating rapid model development and iteration.</li> <li>Flexibility: The library supports a diverse array of model architectures including, but not limited to, GPT, StarCoder, Llama, Mistral, and Mixtral. It is designed to be adaptable, allowing for easy expansion and customization to a broad range of models and training scenarios.</li> <li>Convenience: Designed with the user in mind, Fast-LLM aims to be straightforward and intuitive, enabling researchers and developers to focus more on innovation and less on the complexities of the tooling.</li> </ul>"},{"location":"#project-scope-and-objectives","title":"Project Scope and Objectives","text":"<p>Fast-LLM seeks to provide a high-quality alternative to existing frameworks such as Megatron-LM and NeMo. It is compatible with 3D parallelism and is designed to integrate seamlessly with Huggingface Transformers, promoting not only efficient model training but also straightforward model deployment and inference.</p>"},{"location":"#collaboration-and-contribution","title":"Collaboration and Contribution","text":"<p>The project is set for open-sourcing in Q2 2024, inviting contributions from the community in areas such as testing, bug fixes, new features, and documentation. We are especially interested in enhancements related to custom kernels using OpenAI's Triton JIT compiler and adaptations for alternative hardware platforms like AMD and Intel.</p> <p>For more details on getting involved or using Fast-LLM, please refer to our contribution guidelines and the subsequent sections of this documentation.</p>"},{"location":"license/","title":"License and citations","text":"<p>Fast-LLM is licenced under the Apache 2.0 license:</p> <pre><code>Copyright 2024 ServiceNow, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre>"},{"location":"community/","title":"\ud83d\udc65 Community","text":"<p>Coming soon...</p>"},{"location":"community/feedback/","title":"Feedback","text":"<p>Coming soon...</p>"},{"location":"developers/","title":"Developer Guides","text":"<ul> <li>Contributing: How to contribute to Fast-LLM.</li> </ul>"},{"location":"developers/contribute/","title":"Contributing to Fast-LLM","text":"<p>Coming soon...</p>"},{"location":"reference/","title":"Reference","text":"<p>Coming soon...</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This guide will teach how to pretrain and/or extend pretraining of language models such as Mistral-7B with Fast-LLM on multiple GPU nodes. Such training requires a careful selection and optimization of: - The training hardware: GPU node specs, count and interconnect. - The model architecture: layer types, hidden sizes, activations, etc. - The training dataset and its sampling. - The training parameters: optimizer, learning rate schedule, training duration, etc. - The training performance optimizations: distributed layout, activation recomputation, etc.</p> <p>When training a model with Fast-LLM (and other training libraries), we generally assume the first four points to be predetermined as they are unrelated to the training framework, and focus on the last one, i.e., we optimize a fixed training scheme for throughput. (However, in practice the batch size may be adjusted together with the distributed layout, which in turn affects the training schedule.)</p> <p>In this tutorial, we follow the extended pretraining for Mistral-7B over a corpus of 500 billion tokens using 16 DGX nodes, each equipped with 8 A100 or H100 GPUs (totalling 128 GPUs). We also explore some alternative settings such as training from scratch and the Mixtral-8x7B model.</p> <ul> <li>Getting started: Get started with Fast-LLM, set up and run a first training configuration.</li> <li>Load Mistral-7B: Define the model architecture, download a checkpoint from the Huggingface Hub and load it in Fast-LLM.</li> <li>Prepare and load the dataset: Prepare and configure the dataset.</li> <li>Prepare the training configuration: Configure the optimizer, schedule, distributed layout, etc.</li> <li>Launch and monitor training: Launch training, configure and view experiment outputs.</li> <li>Convert to Hugging Face: Convert to Hugging Face format and upload it to the Hugging Face model hub.</li> </ul>"},{"location":"tutorial/convert_to_huggingface/","title":"Converting Fast-LLM Models to Hugging Face Format","text":"<p>Now that we have trained a Mistral model, the natural next step is to try it for inference or benchmarks. Fast-LLM does not support such task (at least for the time being), but instead supports conversion to Huggingface transformers models, which are themselves compatible with a large variety of tools.</p> <p>This article guides you through the conversion process for a Mistral-7B checkpoint (export) generated during training as described in the previous tutorial. This checkpoint may be found at <code>$EXP_BASE_DIR/export/$ITERATION/</code>. Allow some time for the first checkpoint to be generated.</p>"},{"location":"tutorial/convert_to_huggingface/#convert-a-mistral-7b-checkpoint","title":"Convert a Mistral-7B checkpoint","text":"<p>We convert the checkpoint with Fast-LLM's conversion script, and we specify the input and output locations and formats:</p> <pre><code>python3 -m tools.convert_model \\\n    --input_type distributed \\\n    --output_type huggingface \\\n    --input_path $EXP_BASE_DIR/export/$ITERATION/ \\\n    --output_path $CONVERTED_DIR \\\n    --model_type mistral\n</code></pre> <p>Don't Forget the Tokenizer</p> <p>Make sure to add a tokenizer file and its configuration to the output directory, since <code>convert_model.py</code> does not include these files in the conversion.</p> <p>You can then load and use the converted model as you would with any Transformers model. For example: <pre><code>import torch\nfrom transformers import AutoModelForCausalLM\n\nimport transformers\n\nmodel = AutoModelForCausalLM.from_pretrained(converted_dir).to(device=\"cuda\")\nx = torch.randint(0, 32000, (1, 1024))\ny = model(x)\n</code></pre></p>"},{"location":"tutorial/getting_started/","title":"Getting Started","text":""},{"location":"tutorial/getting_started/#build-the-image","title":"Build the image","text":"<p>Warning</p> <p>This guide is not yet working.</p> <p>The preferred way to run Fast-LLM is through a docker image built with the provided Dockerfile. For example, from a terminal running on a GPU node:</p> <pre><code>git clone git@github.com:ServiceNow/Fast-LLM.git\ncd Fast-LLM\ndocker build -t my_fast_llm_image .\ndocker run --rm -it --gpus all --net=host --ipc=host my_fast_llm_image bash\n</code></pre>"},{"location":"tutorial/getting_started/#first-examples","title":"First examples","text":"<p>All training runs are launched throught the entry point pretrain_fast_llm.py. We can run a minimalistic training example with: <pre><code>python3 pretrain_fast_llm.py --train_iters=100 --batch_size=32 --dataset_source=random\n</code></pre> This will launch a short single-GPU training from scratch of a 180 M parameter model on a randomly generated dataset.</p> <p>To run distributed training, we run our training script through torchrun, the PyTorch distributed launcher. For example, on 8 GPUs: <pre><code>torchrun --nproc-per-node=8 pretrain_fast_llm.py --train_iters=100 --batch_size=32 --dataset_source=random\n</code></pre> Note that by default, Fast-LLM parallelizes over samples (data-parallel), so the number of GPUs should divide the batch size.</p> <p>Multi-node training also uses torchrun, and requires the same command to be run on each node, with the additional specification of a rendez-vous endpoint, i.e., the address of one of the nodes. For example, on four nodes: <pre><code>torchrun --nproc-per-node=8 --nnodes=4 --rdzv-backend=c10d --rdzv-endpoint=$HOST_NODE_ADDR pretrain_fast_llm.py --train_iters=100 --batch_size=32 --dataset_source=random\n</code></pre></p> <p>See the torchrun documentation for more details. Note that if you are using cloud or managed hardware, there Now tutorial](servicenow.md) may be a simpler automated method to launch multi-node jobs. Please refer to your provider for more details. The ServiceNow-specific method may be found in the [Service</p>"},{"location":"tutorial/getting_started/#more-on-training-arguments","title":"More on training arguments","text":"<p>The training script supports hundreds of arguments, though most of them are optional and/or have sensible defaults. We already saw three arguments above, and we will see many important ones in this tutorial.</p> <p>At the beginning of training, Fast-LLM displays a list of arguments and their values: <pre><code>------------------------ arguments ------------------------\n  activation_type ................................. gelu\n  adam_beta1 ...................................... 0.9\n  adam_beta2 ...................................... 0.999\n  adam_eps ........................................ 1e-08\n  add_linear_biases ............................... True\n  attention_dropout ............................... 0.0\n  batch_size ...................................... 1\n  [...]\n-------------------- end of arguments ---------------------\n</code></pre> All of these arguments can be set as arguments of <code>pretrain_fast_llm.py</code>, in the form <code>--[name]=[value]</code>, provided the values have the expected data type, and in some case satisfy extra constraints. For example, we may enable attention dropout with <code>--attention_dropout=0.1</code>. Note that booleans are set as integers (ex. <code>--add_linear_biases=0</code> to disable biases), and that <code>None</code> cannot be represented. Please refer to each parameter's definition for more details.</p>"},{"location":"tutorial/launch_training/","title":"Launch and monitor training","text":""},{"location":"tutorial/launch_training/#requirements","title":"Requirements","text":"<p>At this point, you should already have:</p> <ul> <li>Access to a cluster with 16 DGX nodes with 8x A100/H100-80GB GPUs (Or at least 4 GPUs), connected through an Infiniband (preferred) and/or Ethernet interconnect, and sharing a common fast storage.</li> <li>A docker image for Fast-LLM, available on all nodes.</li> <li>A local copy of the Mistral weights on the common storage</li> <li>A preprocessed dataset in json format on the common storage.</li> <li>(Optional) A Wandb account and API key.</li> </ul>"},{"location":"tutorial/launch_training/#launching-the-experiment","title":"Launching the experiment","text":"<p>To launch the experiment, we perform the following on each node, or use a cluster-specific tool to automate the process: 1. Launch a docker container running our docker image, ensuring access to all necessary hardware (GPUs, interconnects, etc.), and mounting the pretrained weights, dataset and an experiment directory.     <code>bash    docker run --rm -it --gpus all --net=host --ipc=host [-v ...] my_fast_llm_image bash</code> 2. Note the mounted paths and host address:     <pre><code>export PRETRAINED_MISTRAL_PATH=...\nexport JSON_DATA_PATH=...\nexport EXP_BASE_DIR=...\nexport HOST_NODE_ADDR=...\n</code></pre> 3. Set up the experiment configuration as described in the previous sections:     ```bash</p> <pre><code>export ARCHITECTURE_ARGS_MISTRAL_PRETRAINED=\"\\\n</code></pre> <p>--pretrained_checkpoint_type=huggingface \\    --pretrained_checkpoint_path=$PRETRAINED_MISTRAL_PATH \\    \"</p> <p>export MODEL_ARGS_MISTRAL_PRETRAINED=\"\\    $ARCHITECTURE_ARGS_MISTRAL_PRETRAINED \\    --window_size=4096 \\    \"</p> <pre><code>export DATA_ARGS=\"\\\n--split=9998,2,0 \\\n--dataset_source=file \\\n--data_path=$JSON_DATA_PATH \\\n\"\n\nexport TRAINING_ARGS=\"\\\n--batch_size=128 \\\n--sequence_length=8192 \\\n--train_iters=500000 \\\n--weight_decay=0.1 \\\n--adam_beta1=0.9 \\\n--adam_beta2=0.95 \\\n--clip_grad=1.0 \\\n--lr=0.0001 \\\n--lr_warmup_iters=1000 \\\n--lr_decay_style=cosine \\\n--lr_decay_iters=500000 \\\n--min_lr=0.000003 \\\n\"\n\nexport PERFORMANCE_ARGS=\"\\\n--training_dtype=bf16 \\\n--num_workers=8 \\\n\"\n\nexport MONITORING_ARGS=\"\\\n--experiment_dir=$EXP_BASE_DIR \\\n--validation_iters=25 \\\n--validation_interval=1000 \\\n--max_checkpoints=5 \\\n--export_interval=25000 \\\n--log_interval=10 \\\n--log_offset=0 \\\n--checkpoint_interval=500 \\\n\"\n```\n</code></pre> <ol> <li>Launch the experiment:     <code>bash     torchrun --nproc-per-node=8 --nnodes=16 --rdzv-backend=c10d --rdzv-endpoint=$HOST_NODE_ADDR pretrain_fast_llm.py \\     $MODEL_ARGS_MISTRAL_PRETRAINED $DATA_ARGS $TRAINING_ARGS $PERFORMANCE_ARGS $MONITORING_ARGS</code></li> </ol>"},{"location":"tutorial/launch_training/#monitoring-the-experiment","title":"Monitoring the experiment","text":"<p>After launching the experiment, you may observe the progress through either stdout, or the log file at <code>[EXP_BASE_DIR]/runs/0/logs/logs_rank_000.txt</code>. If you set up Wandb logging, progress will also be reported there.</p>"},{"location":"tutorial/prepare_data/","title":"Training Data Preparation","text":""},{"location":"tutorial/prepare_data/#prepare-datasets","title":"Prepare datasets","text":"<p>The data processing of Fast-LLM is designed to closely match that of Megatron-LM. In particular, it requires datasets to be converted to the Megatron-LM binary format. Please refer to this guide for details on how to prepare the dataset(s).</p> <p>At the end of this process, each dataset should have a consist of a binary file <code>$DATA_PREFIX_[i].bin</code> and an index file <code>$DATA_PREFIX_[i].idx</code></p>"},{"location":"tutorial/prepare_data/#list-configuration","title":"List configuration","text":"<p>Datasets may be configured via a simple string in the <code>--data_path</code> argument. (Again, in the exact same format as with Megatron-LM). For a single dataset, we only need to specify its prefix: <pre><code>export DATA_ARGS_SINGLE=\"\\\n--split=9998,2,0 \\\n--dataset_source=list \\\n--data_path=$DATA_PREFIX_0 \\\n\"\n</code></pre> Note that we also specify a train/validation/test split for the dataset. Fow multiple datasets, we specify the prefixes together with relative dataset sampling probabilities. For examples <pre><code>export DATA_ARGS_MULTIPLE=\"\\\n--split=9998,2,0 \\\n--dataset_source=list \\\n--data_path=\\\"0.3 $DATA_PREFIX_0 0.5 $DATA_PREFIX_1 0.2 $DATA_PREFIX_2\\\" \\\n\"\n</code></pre></p> <p>Warning</p> <p>The same dataset split is used for every dataset. This may cause problems for extremely small datasets, which we recommend avoiding. (If needed, we suggest concatenating small datasets into larger ones.)</p> <p>Warning</p> <p>Make sure to dedicate enough data for validation and/or testing, and adjust the split according to you dataset. Our setup assumes a dataset of 500 billion tokens, and requires 26 million tokens for each validation, so allocating 0.02% of the total data (100 million tokens) ensures sufficient data without excessively reducing the training set size.</p>"},{"location":"tutorial/prepare_data/#json-configuration","title":"Json configuration","text":"<p>While the list configuration is sufficient for a small number of datasets, it becomes impractical when there are many of them. For that purpose, Fast-LLM allows configuring a dataset from an external json file.</p> <p>A common use case concerns large datasets with hundreds of billions of tokens, which need to be split into multiple ones to keep the file size reasonable. We want to sample each dataset as if it was not split, i.e. with probability proportional to its document count. In that case, the json configuration file can be generated automatically using the <code>concatenate_dataset.py</code> script: <pre><code>python3 tools/concatenate_dataset.py --directory=$DATASET_DIR --output_name=$JSON_DATA_PATH\n\"\n</code></pre> This script will recursively scan <code>$DATASET_DIR</code> for datasets (<code>.idx</code> files), and create a json dataset configuration at <code>$JSON_DATA_PATH</code> with the appropriate dataset prefixes and probabilities. The resulting json file can be used to configure the datasets: <pre><code>export DATA_ARGS=\"\\\n--split=9998,2,0 \\\n--dataset_source=file \\\n--data_path=$JSON_DATA_PATH \\\n\"\n</code></pre></p> More on the json dataset file <p>The json dataset file is a simple structure for holding the data prefixes and probabilities, to avoid writing them explicitly in the Fast-LLM configuration. It may be created manually or through a script such as <code>concatenate_dataset.py</code> It may also contain metadata about the dataset contents, for example the total number of tokens and documents. The file should be structured as: <pre><code>{\n    \"datasets\": [\n        {\n            \"prefix\": $RELATIVE_DATA_PREFIX_0\"\n            \"weight\": 0.3\n            \"num_documents\": 12345,\n            \"num_tokens\": 987654321,\n            ...\n        },\n        ...\n    ]\n}\n</code></pre> Note that in the json format, paths are relative to the directory containing the json file instead of the current working directory.</p>"},{"location":"tutorial/prepare_mistral/","title":"Load Mistral-7B","text":""},{"location":"tutorial/prepare_mistral/#download-pretrained-weights","title":"Download pretrained weights","text":"<p>Since we are interested in extending the pretraining of Mistral-7B, the first step is to obtain the pretrained weights. We do so by downloading them from the Huggingface Hub. This requires:</p> <ul> <li>Git lfs (<code>git lfs install</code>).</li> <li>An account for the Huggingface Hub, together with an access token.</li> <li>Permission to use Mistral-7B, obtained by accepting the terms and conditions.</li> </ul> <p>Then, clone the repository to download the weights (use the access token as password). <pre><code>git clone https://huggingface.co/mistralai/Mistral-7B-v0.1 $PRETRAINED_CHECKPOINT_PATH\n</code></pre></p>"},{"location":"tutorial/prepare_mistral/#load-the-model-in-fast-llm","title":"Load the model in Fast-LLM","text":"<p>Fast-LLM may load the model architecture and pretrained weights of supported Huggingface models directly at the beginning of training. To do so, we simply specify the pretrained checkpoint format and location, which overrides the model architecture with Mistral-7B. <pre><code>export ARCHITECTURE_ARGS_MISTRAL_PRETRAINED=\"\\\n--pretrained_checkpoint_type=huggingface \\\n--pretrained_checkpoint_path=$PRETRAINED_MISTRAL_PATH \\\n\"\n</code></pre></p> <p>To obtain the full model configuration, we also need to set the non-architecture parameters, which are not imported during conversion.</p> <pre><code>export MODEL_ARGS_MISTRAL_PRETRAINED=\"\\\n$ARCHITECTURE_ARGS_MISTRAL_PRETRAINED \\\n--window_size=4096 \\\n\"\n</code></pre> <p>Warning</p> <p>Make sure to check which model parameters are part of the architecture and which ones are not, and set all required non-architecture parameters explicitly.</p> <p>Warning</p> <p>Make sure the downloaded checkpoint is accessible to every worker, and adjust the path as needed.</p>"},{"location":"tutorial/prepare_mistral/#optional-train-from-scratch","title":"(Optional) Train from scratch","text":"<p>If we want to train a Mistral-7B model from scratch, we may still load the architecture from the Huggingface repo: <pre><code>export ARCHITECTURE_ARGS_MISTRAL_FROM_SCRATCH=\"\\\n--pretrained_checkpoint_type=huggingface \\\n--pretrained_checkpoint_path=$PRETRAINED_CHECKPOINT_PATH \\\n--load_pretrained_weights=0 \\\n\"\n</code></pre></p> <p>Alternatively, we may specify the architecture explicitly, which makes it easier to adjust the parameters. <pre><code>export ARCHITECTURE_ARGS_MISTRAL=\"\\\n--num_layers=32 \\\n--hidden_size=4096 \\\n--vocab_size=32000 \\\n--num_attention_heads=32 \\\n--head_groups=8 \\\n--add_linear_biases=0 \\\n--ffn_hidden_size=14336 \\\n--kv_channels=128 \\\n--use_rotary_embeddings=1 \\\n--rotary_embedding_scale=-9.210340371976184 \\\n--gated=1 \\\n--activation_type=silu \\\n--normalization_type=rms_norm \\\n--tie_word_embeddings=0 \\\n\"\n</code></pre></p> <p>Please refer to the trainer config for additional extended pretraining options.</p>"},{"location":"tutorial/prepare_mistral/#optional-train-mixtral-8x7b","title":"(Optional) Train Mixtral-8x7B","text":"<p>We may train Mixtral-8x7B instead, which simply requires pointing to a different checkpoint:</p> <p><pre><code>git clone https://huggingface.co/mistralai/Mistral-7B-v0.1Mixtral-8x7B-v0.1 $PRETRAINED_CHECKPOINT_PATH\n</code></pre> Other than a small memory optimization, this tutorial can be run as-is with Mixtral-8x7B. The architecture is a slight vatiation of Mistral-7B: <pre><code>export ARCHITECTURE_ARGS_MIXTRAL=\"\\\n$ARCHITECTURE_ARGS_MISTRAL \\\n--num_experts=8 \\\n--num_experts_per_token=2 \\\n\"\n</code></pre></p>"},{"location":"tutorial/prepare_training/","title":"Prepare the training configuration","text":""},{"location":"tutorial/prepare_training/#training-parameters","title":"Training parameters","text":"<p>Our example training scheme is as follows: 1. We train over 500 K iteration, each made of 128 samples of 8192 tokens, for a total of 524 B training tokens. 2. We use the Adam optimizer with weight decay (Adamw), and gradient clipping. 3. We warm up the learning rate for the first 1000 steps, then use cosine decay from 1e-4 to 3e-6.</p> <p>This translates into the following Fast-LLM configuration: <pre><code>export TRAINING_ARGS=\"\\\n--batch_size=128 \\\n--sequence_length=8192 \\\n--train_iters=500000 \\\n--weight_decay=0.1 \\\n--adam_beta1=0.9 \\\n--adam_beta2=0.95 \\\n--clip_grad=1.0 \\\n--lr=0.0001 \\\n--lr_warmup_iters=1000 \\\n--lr_decay_style=cosine \\\n--lr_decay_iters=500000 \\\n--min_lr=0.000003 \\\n\"\n</code></pre></p>"},{"location":"tutorial/prepare_training/#performance-parameters","title":"Performance parameters","text":"<p>Our training setup is simple enough that the default distributed configuration (data parallel with ZeRO stage 1) is sufficient for a near-optimal training throughput of around 9000 tokens/s/GPU on H100 GPUs (440 tflops/GPU). We only need to specify the training dtype and the number of data loader workers. <pre><code>export PERFORMANCE_ARGS=\"\\\n--training_dtype=bf16 \\\n--num_workers=8 \\\n\"\n</code></pre></p> <p>Note that this configuration requires exactly 16 nodes. It may be adjusted run on fewer than 16 nodes, by using gradient accumulation to keep the micro-batch size constant and adding some memory optimizations. We suggest the following configuration for 4 to 64 GPUs (seet details the in next section): <pre><code>export PERFORMANCE_ARGS_SMALL_CLUSTER=\"\\\n$PERFORMANCE_ARGS \\\n--micro_batch_size=1 \\\n--zero_stage=2 \\\n\"\n</code></pre></p>"},{"location":"tutorial/prepare_training/#optional-more-on-mistral-performance-optimization","title":"(Optional) More on Mistral performance optimization","text":"<p>The performance optimization of Mistral at the configuration level is mainly determined through the following guidelines:</p> <ul> <li>Use larger micro-batches: The GPU runs more efficiently with larger kernels, so we want the micro-batches to be as large as allowed by memory and other constraints. Our configuration requires 36 GiB of activation memory, so a micro-batch or 8192 tokens per GPU is a reasonable choice. A value of 16384 tokens per GPU is technically feasible, but would require aggressive state memory optimizations and a higher batch size. 2 - Reduce model parallelism: Model parallelism (tensor or pipeline) comes with a large overhead, so we should avoid or limit it whenever possible. For Mistral, no model parallelism is needed. 3 - Optimize the memory usage: Additional memory optimizations are available to enable configurations that would otherwise not be possible. We already saw the most important one, the ZeRO stage (<code>--zero_stage</code> see note below). An additional one is the recomputation of the MLP activations <code>--mlp_recompute_level</code> , which significantly lower the activation memory usage, for a small (<code>activation</code>) or moderate (<code>full</code>) overhead. Note that Fast-LLM does not implement activation recomputation for the entire transformer layer, as it comes with a large overhead (~33%) and it can be avoided in (almost) all practical scenario.</li> </ul> More on ZeRO stages <p>Fast-LLM provides a custom implementation of the training state partitioning first described in the ZeRO (Zero Redundancy Optimizer) paper. The method comes in three \"stages\", which progressively reduce the memory footprint from the training state:</p> <ul> <li> <p>Stage 1: Partition the optimizer state and its update across the data-parallel GPUs.   This stage reduces the state memory by around 3x (for mixed precision training with full-precision gradients),   while simultanuously speeding up training through a faster weight update.</p> </li> <li> <p>Stage 2: Extend partitioning to the (reduced) gradients.   This stage reduces the state memory by a further 3x,   but may come with a minor overhead (depending on the implementation),   and may require multiple reductions with gradient accumulation.</p> </li> <li> <p>Stage 3: Extend partitioning to the weights.   This stage drops the vast majority of the remaining state memory,   but requires extra network communication.</p> </li> </ul> <p>Fast-LLM implements all three of these stages, selected through the <code>--zero_stage</code> argument. There is no option to disable ZeRO entirely, as it would be strictly worse in terms of performance. In general, training configurations should use the lowest value allowed by other memory constraints.</p> Recompute Level for MLPs <p>The MLP is the largest contributor to a transformer's activation memory (with Flash Attention), so recomputing its activations is a natural way to save memory. Fast-LLM offers three MLP recomputaton modes, set throught the <code>--mlp_recompute_level</code> argument:</p> <ul> <li> <p><code>none</code> (default): All MLP activations are kept, allowing for the highest throughput at the highest memory cost.</p> </li> <li> <p><code>activation</code>: The MLP activation layer output (gelu, silu, etc.) is dropped and recomputed in the backward pass. This saves on activation memory (~20% for Mistral) with minimal impact on throughput.</p> </li> <li> <p><code>full</code>: Both the first dense layer and activation layer outputs are dropped and recomputed. This saves more activation memory (~60% for Mistral), but has a noticeable impact on throughput .</p> </li> </ul> <p>For quantitative comparison, here are benchmarks for Mistral (using 4x A100 GPUs):</p> Recompute Level Act. Memory (MiB) Tokens/s/GPU Model TFLOP/s/GPU <code>none</code> 36515 4234.09 202.88 <code>activation</code> 29346 4218.63 202.14 <code>full</code> 15010 3804.49 182.29"},{"location":"tutorial/prepare_training/#monitoring-and-persistence-parameters","title":"Monitoring and persistence parameters","text":"<p>Finally, we set up experiment monitoring and persistence <pre><code>export MONITORING_ARGS=\"\\\n--experiment_dir=$EXP_BASE_DIR \\\n--validation_iters=25 \\\n--validation_interval=1000 \\\n--max_checkpoints=5 \\\n--export_interval=25000 \\\n--log_interval=10 \\\n--log_offset=0 \\\n--checkpoint_interval=500 \\\n\"\n</code></pre> This setup includes: - Creation of an experiment directory at <code>$EXP_BASE_DIR</code> to store checkpoints, logs, data cache and other artifacts. - Validation for 25 steps every 1000 steps - Logging of losses, metrics and other relevant quantities every 10 steps (from rank 0),   both to stdout and the log file. - Saving of a temporary checkpoint every 500 steps, and of a permanent checkpoint every 25000 steps.</p> More on Fast-LLM checkpointing <p>Fast-LLM provides two types of checkpoints:</p> <ul> <li><code>checkpoint</code>: temporary checkpoint saved at <code>[--experiment_dir]/checkpoints/[iter]</code>,   to reload the experiment in case of a planned or unexpected shutdown.   Only the <code>--max_checkpoints</code> most recent ones are kept to limit disk usage.   Note that saving a checkpoint with Fast-LLM is relatively fast so can (and should) be done frequently.</li> <li><code>export</code>: permanent checkpoint saved at <code>[--experiment_dir]/export/[iter]</code>.   This checkpoint type is typically intended for long-term storage, benchmarking, inference, etc.   It should be saved less often to limit disk usage.</li> </ul>"},{"location":"tutorial/prepare_training/#optional-set-up-wandb","title":"(Optional) Set up wandb","text":"<p>Fast-LLM also support monitoring through Weights and Biases. This requires a valid API key, passed through an environment variable rather than an explicit argument for security reasons. It can be either contained in <code>$WANDB_API_KEY</code> or in a plain text file found at <code>$WANDB_API_KEY_PATH</code>. Then, we set the Wandb username, project and version (Wandb group). <pre><code>export WANDB_ARGS=\"\\\n--wandb_entity_name=$WANDB_ENTITY_NAME \\\n--wandb_project_name=$PROJECT_NAME \\\n--wandb_group_name=$PROJECT_VERSION \\\n\"\n</code></pre> The Wandb run will be set as the directory name of <code>$EXP_BASE_DIR</code>, or can be overriden through <code>--experiment_name</code>.</p>"},{"location":"blog/","title":"Blog","text":""}]}